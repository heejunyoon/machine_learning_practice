{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM\n",
    "from keras.optimizers import adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data, label data 생성\n",
    "라벨 데이터 생성 함수 실행 전에 label_512.csv 삭제 후 진행할 것. 조건문 활용 코드 수정 요망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data 생성\n",
    "def load_train_data(train_data_num, train_data_path):\n",
    "    \n",
    "    #train_data_num=input('csv file 몇개?')\n",
    "    #train_data_path=input('path?')\n",
    "    filename = train_data_path+'/dataset_512_csv1.csv'\n",
    "    train_data=np.loadtxt(filename,delimiter=',')\n",
    "    \n",
    "    for i in range(2,int(train_data_num)+1):\n",
    "        filename = train_data_path + '/dataset_512_csv'+str(i)+'.csv'\n",
    "        train_data=np.append(train_data,np.loadtxt(filename,delimiter=','),axis=0)\n",
    "    \n",
    "    frame = list(range(0,300))\n",
    "    train_data_df = pd.DataFrame(train_data, columns=frame)\n",
    "\n",
    "    return train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label 생성\n",
    "def load_label_data():\n",
    "    train_data_num = input('csv file 몇개?')\n",
    "    train_data_path = input('path?')\n",
    "    \n",
    "    label_512=open(train_data_path + \"/label_512.csv\",\"a\")\n",
    "    csv_label = pd.read_csv(train_data_path + '/label.csv')\n",
    "    label_512.write('label\\n')\n",
    "\n",
    "    for a in range(0, int(train_data_num)): #데이터 파일 갯수\n",
    "        each_label = csv_label.iloc[a,0]\n",
    "        for b in range(0, 512): #각 파일당 행 갯수(고정)\n",
    "                 label_512.write(str(each_label))\n",
    "                 label_512.write('\\n')\n",
    "    label_data_df = pd.read_csv(train_data_path + \"/label_512.csv\")\n",
    "    \n",
    "    return label_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1 = load_train_data(22, \"C:/Users/DSPL/Desktop/딥러닝 스터디 발표/data/0804/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2 = load_train_data(20, \"C:/Users/DSPL/Desktop/딥러닝 스터디 발표/data/0731/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1 = load_label_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2 = load_label_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11264, 1)\n",
      "(10240, 1)\n",
      "(21504, 300)\n",
      "(21504, 1)\n"
     ]
    }
   ],
   "source": [
    "#생성 데이터들 합치기(train, test, validation 한번에 나누기 위함)\n",
    "label_1 = pd.read_csv('C:/Users/DSPL/Desktop/딥러닝 스터디 발표/data/0804/dataset/label_512.csv') #자꾸 오류나서 그냥 다시 로드\n",
    "label_2 = pd.read_csv('C:/Users/DSPL/Desktop/딥러닝 스터디 발표/data/0731/dataset/label_512.csv')\n",
    "\n",
    "all_data = pd.concat([train_set1, train_set2], ignore_index=True)\n",
    "all_label = pd.concat([label_1, label_2], ignore_index=True)\n",
    "\n",
    "print(label_1.shape)\n",
    "print(label_2.shape)\n",
    "print(all_data.shape)\n",
    "print(all_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, validation, test set 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_data\n",
    "target = all_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13762, 300) (4301, 300) (3441, 300) (13762, 1) (4301, 1) (3441, 1)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=1)\n",
    "\n",
    "data_train, data_val, target_train, target_val = train_test_split(data_train, target_train, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "print(data_train.shape, data_test.shape, data_val.shape, target_train.shape, target_test.shape, target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13762, 300, 1) (13762, 1) (3441, 300, 1)\n"
     ]
    }
   ],
   "source": [
    "#(13762, 300) (4301, 300) (3441, 300) (13762, 1) (4301, 1) (3441, 1)\n",
    "\n",
    "data_train = np.expand_dims(data_train,axis=-1)\n",
    "data_test = np.expand_dims(data_test,axis=-1)\n",
    "data_val = np.expand_dims(data_val, axis=-1)\n",
    "print(data_train.shape, target_train.shape, data_val.shape)\n",
    "n_timesteps, n_features, n_outputs =data_train.shape[1], data_train.shape[2],target_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch normalization 4th model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch 네번째 \n",
    "#padding 추가\n",
    "model_batch_norm4 = tf.keras.Sequential()\n",
    "\n",
    "#hidden1\n",
    "model_batch_norm4.add(tf.keras.layers.Conv1D(filters=16, kernel_size=3, padding='valid', input_shape=(n_timesteps,n_features)))\n",
    "model_batch_norm4.add(tf.keras.layers.BatchNormalization())\n",
    "model_batch_norm4.add(tf.keras.layers.ReLU())    \n",
    "model_batch_norm4.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='valid'))# Add Batchnorm layer before Activation\n",
    "\n",
    "   \n",
    "model_batch_norm4.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='valid'))\n",
    "model_batch_norm4.add(tf.keras.layers.BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "model_batch_norm4.add(tf.keras.layers.ReLU())    \n",
    "model_batch_norm4.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    \n",
    "    \n",
    "model_batch_norm4.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='valid'))\n",
    "model_batch_norm4.add(tf.keras.layers.BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "model_batch_norm4.add(tf.keras.layers.ReLU())\n",
    "\n",
    "\n",
    "model_batch_norm4.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='valid'))\n",
    "model_batch_norm4.add(tf.keras.layers.BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "model_batch_norm4.add(tf.keras.layers.ReLU())\n",
    "    \n",
    "    \n",
    "model_batch_norm4.add(tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid'))\n",
    "model_batch_norm4.add(tf.keras.layers.BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "model_batch_norm4.add(tf.keras.layers.ReLU())\n",
    "model_batch_norm4.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "\n",
    "\n",
    "\n",
    "model_batch_norm4.add(tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid'))\n",
    "model_batch_norm4.add(tf.keras.layers.BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "model_batch_norm4.add(tf.keras.layers.ReLU())\n",
    "\n",
    "\n",
    "\n",
    "model_batch_norm4.add(tf.keras.layers.Flatten())\n",
    "model_batch_norm4.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model_batch_norm4.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model_batch_norm4.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "\n",
    "    \n",
    "# Compile model\n",
    "model_batch_norm4.compile(optimizer=keras.optimizers.Adam(lr=0.0001,epsilon=None, decay=0.0, amsgrad=False),\n",
    "                  loss='mse', metrics=['mean_absolute_error'])\n",
    "\n",
    "log_dir = \"logs\\\\fit_batch\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_batch_norm4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import pydot\n",
    "import pydotplus\n",
    "from pydotplus import graphviz\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "keras.utils.vis_utils.pydot = pydot\n",
    "tf.keras.utils.plot_model(model_batch_norm4, to_file='model_combined.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_batch_norm4.fit(x = data_train, y = target_train, batch_size=64, epochs=200, shuffle=True,\n",
    "                    validation_data=(data_val, target_val), \n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#epoch 200에서 300으로 증가함.\n",
    "\n",
    "history4_1 = model_batch_norm4.fit(x = data_train, y = target_train, batch_size=64, epochs=300, shuffle=True,\n",
    "                    validation_data=(data_val, target_val), \n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_batch_norm4.evaluate(data_test, target_test, batch_size=64)\n",
    "print(\"test loss, mean_absolute_error:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2 = model_batch_norm4.evaluate(data_test, target_test, batch_size=64)\n",
    "print(\"test loss, mean_absolute_error:\", score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_batch_norm4.save('model_batch_norm4_epoch300.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model mean_absolute_error')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
